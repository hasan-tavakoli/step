version: "3.7"
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    networks:
      - localnet
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:latest
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    networks:
      - localnet
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENERS: LISTENER_1://broker:29092,LISTENER_2://broker:9092
      KAFKA_ADVERTISED_LISTENERS: LISTENER_1://broker:29092,LISTENER_2://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_1:PLAINTEXT,LISTENER_2:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"
      KAFKA_DELETE_TOPIC_ENABLE: "true"

  connect:
    image: cdc-tutorial-connect-1.6.1:1.0
    build:
      context: .
      dockerfile: mongodb/Dockerfile-MongoConnect
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper
      - broker
    networks:
      - localnet
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "broker:29092"
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-cluster-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_ZOOKEEPER_CONNECT: "zookeeper:2181"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_AUTO_CREATE_TOPICS_ENABLE: "true"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"

  mongo1:
    image: "cdc-tutorial-mongod:1.0"
    container_name: mongo1
    build:
      context: .
      dockerfile: mongodb/ Dockerfile-Mongo
    command: --replSet rs0
    networks:
      - localnet
    restart: always

  mongo1-setup:
    image: "cdc-tutorial-mongod:1.0"
    container_name: mongo1-setup
    build:
      context: .
      dockerfile: mongodb/Dockerfile-Mongo
    depends_on:
      - mongo1
    networks:
      - localnet
    entrypoint:
      [
        "bash",
        "-c",
        "sleep 10 && mongo --host mongodb://mongo1:27017 config-replica.js && sleep 10 && mongo --host mongodb://mongo1:27017 config-data.js && mongoimport --host mongo1 --db mydb --collection user --type json --file /user.json --jsonArray && mongoimport --host mongo1 --db mydb --collection step --type json --file /step.json --jsonArray && mongoimport --host mongo1 --db mydb --collection bloodpressure --type json --file /bloodpressure.json --jsonArray",
      ]
    restart: "no"

  shell:
    image: "cdc-tutorial-shell:1.0"
    container_name: shell
    build:
      context: .
      dockerfile: shell/Dockerfile-shell
    depends_on:
      - zookeeper
      - broker
      - connect
      - mongo1
    networks:
      - localnet
    command: bash -c "./initialize-container.sh ; tail -f /dev/null"

  namenode:
    image: kadensungbincho/ranger-hadoop-namenode:2.0.0-hadoop3.2.1-java8-ranger2.1.0
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./hdfs/install.properties:/opt/ranger-hdfs-plugin/install.properties
    environment:
      - CLUSTER_NAME=test
    env_file:
      - hadoop.env
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    hostname: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    hostname: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - hadoop.env

  hive-server:
    image: kadensungbincho/ranger-hive:2.0.0-hadoop3.2.1-java8-ranger2.1.0
    container_name: hive-server
    hostname: hive-server
    volumes:
      - ./hive/install.properties:/opt/ranger-hive-plugin/install.properties
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"

  hive-metastore:
    image: kadensungbincho/ranger-hive:2.0.0-hadoop3.2.1-java8-ranger2.1.0
    container_name: hive-metastore
    hostname: hive-metastore
    env_file:
      - ./hadoop.env
    command: ["/run.sh", "metastore"]
    environment:
      SERVICE_PRECONDITION: "namenode:9000 datanode:9864 hive-db:3306"
    ports:
      - "9083:9083"

  hive-db:
    image: mysql:8.0.21
    container_name: hive-db
    hostname: hive-db
    command: --default-authentication-plugin=mysql_native_password --lower-case-table-names=1
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: metastore
      MYSQL_USER: hive
      MYSQL_PASSWORD: hive

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:

networks:
  localnet:
    attachable: true
